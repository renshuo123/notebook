

# 高可用⭐

大型互联网架构设计，讲究一个`四件套`组合拳玩法，`高并发`、`高性能`、`高可用`、`高扩展`。

如果能掌握这四个方面，应付大厂面试以及日常工作中的架构方案设计基本不是什么难题。

今天，Tom哥就带大家学习下`高可用`都有哪些设计技巧？

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072124817.png" alt="image-20220707212442710" style="zoom:50%;" />



## 一、系统拆分 

有句古话 "牵一发而动全身"。

面对一个庞然大物，如果没有一个合理的分工分层。任何一个小小失误都会被无限放大，酿成巨大灾难。

万物相通，回到我们的软件架构。

早前的系统都是单体系统，比如电商业务，会员、商品、订单、物流、营销等模块都堆积在一个系统。每到节假日搞个大促活动，系统扩容时，一扩全扩，一挂全挂。只要一个接口出了问题，整个系统都不可用。

“鸡蛋不能放在一个篮子里”，这种连带风险换谁都承受不起。

因此，`系统拆分` 成了更多人的选择。

慢慢的就有了我们现在看到的`微服务`架构，将一个复杂的业务域按DDD的思想拆分成若干子系统，每个子系统负责专属的业务功能，做好垂直化建设，各个子系统之间做好边界隔离，降低风险蔓延。

## 二、解耦

软件开发有个重要原则“高内聚、低耦合”。

小到`接口抽象`、`MVC 分层`，大到 `SOLID 原则`、`23种设计模式`。核心都是降低不同模块间的耦合度，避免一处错误改动影响到整个系统。

就以`开闭原则`为例，对扩展是开放的，对修改是关闭的。随着业务功能迭代，如何做到每次改动不对原来的旧代码产生影响。

Spring 框架给我们提供了一个很好的思路，里面有个重要设计 `AOP` ，全称（Aspect Oriented Programming），面向切面编程。

核心就是采用动态代理技术，通过对字节码进行增强，在方法调用的时候进行拦截，以便于在方法调用前后，增加我们需要的额外处理逻辑。

当然还有一个重要思路就是`事件机制`，通过`发布订阅模式`，新增的需求，只需要订阅对应的`事件通知`，针对性消费即可。不会对原来的代码侵入性修改，是不是会好很多。

## 三、异步

同步指一个进程在执行请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去。

效率会大大降低，聪明的人想到了 `异步` 方式。

如果是非实时响应的动作可以采用异步来完成，线程不需要一直等待，而是继续执行后面的逻辑。

如：线程池（ThreadPoolExecutor）、消息队列 等都是这个原理

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072125887.png" alt="image-20220707212540810" style="zoom:67%;" />

比如一个用户在淘宝下了一笔购物订单，关心的是订单是否创建成功，能否进行后续的付款流程

至于其他业务动作，如短信通知、邮件通知、生成订单快照、创建超时任务记录，这些非核心动作用户并不是特别关心。

我们可以采用消息队列的`发布/订阅` 机制，数据库插入订单记录后，发布一条消息到 MQ，然后就可以告知用户下单成功。

其他事情，由不同的 Task 任务订阅消息异步处理，彼此间互不干扰。

## 四、重试

重试主要是体现在远程的RPC调用，受 `网络抖动`、`线程资源阻塞` 等因素影响，请求无法及时响应。

为了提升用户体验，调用方可以通过 `重试` 方式再次发送请求，尝试获取结果。比过：浏览器的 F5 刷新机制就是类似道理。

接口重试是一把双刃剑，虽然客户端收到了`响应超时`结果，但是我们无法确定，服务端是否已经执行完成。如果盲目地重试，可能会带来严重后果。比如：银行转账。

`重试`通常跟`幂等`组合使用，如果一个接口支持了 `幂等`，那你就可以随便重试

关于的 `幂等` 的解决方案

- 插入前先执行查询操作，看是否存在，再决定是否插入
- 增加唯一索引
- 建防重表
- 引入状态机，比如付款后，订单状态调整为`已付款`，SQL 更新记录前 增加条件判断
- 增加分布式锁
- 采用 Token 机制，服务端增加 token 校验，只有第一次请求是合法的

## 五、补偿

我们知道不是所有的请求都能收到成功响应。除了上面的 `重试` 机制外，我们还可以采用补偿玩法，实现数据`最终一致性`。

业务补偿根据处理的方向分为两部分：

- 正向。多个操作构成一个分布式事务，如果部分成功、部分失败，我们会通过最大努力机制将`失败`的任务推进到成功状态
- 逆向。同上道理，我们也可以采用反向操作，将部分成功任务恢复到`初始状态`

> 注意：补偿操作有个重要前提，业务能接受短时间内的数据不一致。

补偿有很多的实现方式：

1、本地建表方式，存储相关数据，然后通过定时任务扫描提取，并借助反射机制触发执行

2、也可以采用简单的消息中间件，构建业务消息体，由下游的的消费任务执行。如果失败，可以借助MQ的重试机制，多次重试

## 六、备份

任何服务器都有宕机的可能性，一旦存储了数据，带上状态，如果发生故障，数据丢失，后果是我们无法承受的。

所以，`容灾备份`也就变成了互联网的基本能力。

那如何备份，不同的框架有不用的玩法。我们以 Redis 为例：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072127472.png" alt="image-20220707212747377" style="zoom:50%;" />

Redis 借助 `RDB` 和 `AOF` 来实现两台服务器间的数据同步

- RDB，全量数据同步
- AOF，增量数据同步，回放日志

一旦主节点挂了怎么办？

这里引入哨兵机制。哨兵机制可以实现主从库的自动切换，有效解决了故障转移。整个过程分为三个阶段：监控、选主、通知。

除了 Redis 中间件外，其他常见的 MySQL、Kafka 消息中间件、HBase 、ES 等 ，凡是涉及到数据存储的介质，都有备份机制，一旦主节点挂了，会启用备份节点，保证数据不会丢失。

## 七、多活策略

虽然有了上面的`备份`策略，那是不是就万事大吉呢？

在一些极端情况，如：机房断电、机房火灾、地震、山洪等不可抗力因素，所有的服务器都可能出现故障，无法对外提供服务，导致整体业务瘫痪。

为了降低风险，保证服务的24小时可用性，我们会采用 `多活策略`。

常见的`多活`方案有，`同城双活`、`两地三中心`、`三地五中心`、`异地双活`、`异地多活`

不同的方案技术要求、建设成本、运维成本也都不一样。

多活的技术方案复杂，需要考虑的问题点也非常多，这里只是抛砖引玉就不过多展开

## 八、隔离

隔离属于物理层面的分割，将若干的系统低耦合设计，独立部署，从物理上隔开。

每个子系统有自己独立的代码库，独立开发，独立发布。一旦出现故障，也不会相互干扰。当然如果不同子系统间有相互依赖，这种情况比较特殊，需要有默认值或者异常特殊处理，这属于业务层面解决方案。

隔离属于分布式技术的衍生产物，我们最常见的微服务解决方案。

将一个大型的复杂系统拆分成若干个微服务系统，这些微服务子系统通常由不同的团队开发、维护，独立部署，服务之间通过 `RPC` 远程调用。

隔离使得系统间边界更加清晰，故障可以更加隔离开来，问题的发现与解决也更加快速，系统的可用性也更高。

## 九、限流

高并发系统，如果遇到流量洪峰，超过了当前系统的承载能力。我们要怎么办？

一种方案，照单全收，CPU、内存、Load负载飚的很高，最后处理不过来，所有请求都超时无法正常响应。

另一种解决方案，“舍得，有舍有得”，多余的流量我们直接丢弃。

限流定义：

> 限制到达系统的并发请求数量，保证系统能够正常响应部分用户请求，而对于超过限制的流量，则通过拒绝服务的方式保证整体系统的可用性。

**根据作用范围：限流分为单机版限流、分布式限流**

1、单机版限流

主要借助于本机内存来实现计数器，比如通过AtomicLong#incrementAndGet()，但是要注意之前不用的key定期做清理，释放内存。

纯内存实现，无需和其他节点统计汇总，性能最高。但是优点也是缺点，无法做到全局统一化的限流。

2、分布式限流

单机版限流仅能保护自身节点，但无法保护应用依赖的各种服务，并且在进行节点扩容、缩容时也无法准确控制整个服务的请求限制。而分布式限流，以集群为维度，可以方便的控制这个集群的请求限制，从而保护下游依赖的各种服务资源。

**限流支持多个维度：**

- 整个系统一定时间内（比如每分钟）处理多少请求
- 单个接口一定时间内处理多少流量
- 单个IP、城市、渠道、设备id、用户id等在一定时间内发送的请求数
- 如果是开放平台，则为每个appkey设置独立的访问速率规则

**常见的限流算法：**

- 计数器限流
- 滑动窗口限流
- 漏桶限流
- 令牌桶限流

## 十、熔断

关键字：`断路保护`。比如 A 服务调用 B 服务，由于网络问题或 B 服务宕机了或 B 服务的处理时间长，导致请求的时间超长，如果在一定时间内多次出现这种情况，就可以直接将 B 断路了（A 不再请求B）。而调用 B 服务的请求直接返回降级数据，不必等待 B 服务的执行。因此 B 服务的问题，不会级联影响到 A 服务。

熔断，其实是对调用链路中某个资源出现不稳定状态时（如：调用超时或异常比例升高），对这个资源的调用进行限制，让请求快速失败，避免影响到其它的资源而导致级联错误。

熔断的主要方式是使用断路器阻断对故障服务器的调用

断路器有三种状态，关闭、打开、半打开。

**状态机：**

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072129080.png" alt="image-20220707212916989" style="zoom:67%;" />

1、关闭（Closed）状态：在这个状态下，请求都会被转发给后端服务。同时会记录请求失败的次数，当请求失败次数在一段时间超过一定次数就会进入打开状态。

2、打开（Open）状态：在这个状态下，熔断器会直接拒绝请求，返回错误，而不去调用后端服务。同时，会有一个定时器，时间到的时候会变成半打开状态。目的是假设服务会在一段时间内恢复正常。

3、半打开（Half Open）状态：在这个状态下，熔断器会尝试把部分请求转发给后端服务，目的是为了探测后端服务是否恢复。如果请求失败会进入打开状态，成功情况下会进入关闭状态，同时重置计数。

目前，市面流行的解决方案是阿里的开源框架 `Sentinel`，提供了Dashboard控制台用于定义资源以及规则配置

## 十一、降级

降级是系统保护的一种重要手段。

正如 “好钢用在刀刃上”，为了使`有限资源`发挥最大价值，我们会临时关闭一些非核心功能，减轻系统压力，并将有限资源留给核心业务。

比如电商大促，业务在峰值时刻，系统抵挡不住全部的流量时，系统的负载、CPU 的使用率都超过了预警水位，可以对一些非核心的功能进行降级，降低系统压力，比如把`商品评价`、`成交记录`等功能临时关掉。弃车保帅，保证 `创建订单`、`订单支付` 等核心功能的正常使用。

当然，不同业务、不同公司，处理方式也各不相同，需要结合实际场景，和业务方同学一块讨论，最后达成一个统一认可的降级方案。

降级数据可以简单理解为快速返回了一个 false，前端页面告诉用户“服务器当前正忙，请稍后再试。”

**总结下来：降级是通过暂时关闭某些非核心服务或者组件从而保护核心系统的可用性。**

**熔断和降级的相同点？**

- 熔断和限流都是为了保证集群大部分服务的可用性和可靠性。防止核心服务崩溃
- 给终端用户的感受就是某个功能不可用。

**熔断和降级的不同点？**

- 熔断是被调用方出现了故障，主动触发的操作。
- 降级是基于全局考虑，停止某些正常服务，释放资源。

- 

# 异地多活策略⭐

[搞懂异地多活，看这篇就够了 (qq.com)](https://mp.weixin.qq.com/s?__biz=MzkwNjMwMTgzMQ==&mid=2247490455&idx=1&sn=74013dfc33c7d46626dca1a7ae048419&chksm=c0ebc37ff79c4a699b15040002f35ee7993dcf002cde34c2599234110369ea4f4732854c5f75&mpshare=1&scene=23&srcid=0707hWPH2XI0K8OPVhubL1o6&sharer_sharetime=1657153405654&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

在软件开发领域，「异地多活」是分布式系统架构设计的一座高峰，很多人经常听过它，但很少人理解其中的原理。

**异地多活到底是什么？为什么需要异地多活？它到底解决了什么问题？究竟是怎么解决的？**

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072133478.png" alt="image-20220707213351421" style="zoom:50%;" />



## 01 系统可用性⭐

要想理解异地多活，我们需要从架构设计的原则说起。

现如今，我们开发一个软件系统，对其要求越来越高，如果你了解一些「架构设计」的要求，就知道一个好的软件架构应该遵循以下 3 个原则：

1. 高性能
2. 高可用
3. 易扩展

其中，高性能意味着系统拥有更大流量的处理能力，更低的响应延迟。例如 1 秒可处理 10W 并发请求，接口响应时间 5 ms 等等。

易扩展表示系统在迭代新功能时，能以最小的代价去扩展，系统遇到流量压力时，可以在不改动代码的前提下，去扩容系统。

而「高可用」这个概念，看起来很抽象，怎么理解它呢？通常用 2 个指标来衡量：

- **平均故障间隔 MTBF**（Mean Time Between Failure）：表示两次故障的间隔时间，也就是系统「正常运行」的平均时间，这个时间越长，说明系统稳定性越高
- **故障恢复时间 MTTR**（Mean Time To Repair）：表示系统发生故障后「恢复的时间」，这个值越小，故障对用户的影响越小

可用性与这两者的关系：

> 可用性（Availability）= MTBF / (MTBF + MTTR) * 100%

### N个9分析⭐

这个公式得出的结果是一个「比例」，通常我们会用「N 个 9」来描述一个系统的可用性。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072134403.png" alt="image-20220707213432340" style="zoom:67%;" />

从这张图你可以看到，要想达到 4 个 9 以上的可用性，平均每天故障时间必须控制在 10 秒以内。

来反观下 B 站故障了多久，2021-07-13 23:00 到 2021-07-14 02:00，系统逐渐恢复，如果按照年故障总时间来算的话：B 站故障超过 1 个小时了，只能算达到了三个九的标准。如果按照日故障时间来算，只能达到两个九的标准，也就是 99% 的高可用性，有点惨...

**1 一个九和两个九**

非常容易达到，一个正常的线上系统不会每天宕机 15 分钟吧，不然真用不下去了。

**2 三个九和四个九**

允许故障的时间很短，年故障时间是 1 小时到 8 小时，需要从架构设计、代码质量、运维体系、故障处理手册等入手，其中非常关键的一环是运维体系，如果线上出了问题，第一波收到异常通知的肯定是运维团队，根据问题的严重程度，会有不同的运维人员来处理，像 B 站这种大事故，就得运维负责人亲自上阵了。

另外在紧急故障发生时，是否可以人工手段降级或者加开关，限制部分功能，也是需要考虑的。之前我遇到过一个问题，二维码刷卡功能出现故障，辛亏之前做了一个开关，可以将二维码功能隐藏，如果用户要使用二维码刷卡功能，统一引导用户走线下刷卡功能。

**3 五个九**

年故障时间 5 分钟以内，这个相当短，即使有强大的运维团队每天值班也很难在收到异常报警后，5 分钟内快速恢复，所以只能用自动化运维来解决。也就是服务器自己来保证系统的容灾和自动恢复的能力。

**4 六个九**

这个标准相当苛刻了，年故障时间 32 秒。

针对不同的系统，其实对几个九也不相同。比如公司内部的员工系统，要求四个九就可以，如果是给全国用户使用，且使用人数很多，比如某宝、某饿，那么就要求五个九以上了，但是即使是数一数二的电商系统，它里面也有非核心的业务，其实也可以放宽限制，四个九足以，这个就看各家系统的要求，都是成本、人力、重要程度的权衡考虑。

也就是说，只有故障的时间「越短」，整个系统的可用性才会越高，每提升 1 个 9，都会对系统提出更高的要求。

### 故障体现方面

我们都知道，系统发生故障其实是不可避免的，尤其是规模越大的系统，发生问题的概率也越大。这些故障一般体现在 3 个方面：

1. **硬件故障**：CPU、内存、磁盘、网卡、交换机、路由器
2. **软件问题**：代码 Bug、版本迭代
3. **不可抗力**：地震、水灾、火灾、战争

这些风险随时都有可能发生。所以，在面对故障时，我们的系统能否以「**最快**」的速度恢复，就成为了可用性的关键。

可如何做到快速恢复呢？

这篇文章要讲的「异地多活」架构，就是为了解决这个问题，而提出的高效解决方案。

下面，我会从一个最简单的系统出发，带你一步步演化出一个支持「异地多活」的系统架构。

在这个过程中，你会看到一个系统会遇到哪些可用性问题，以及为什么架构要这样演进，从而理解异地多活架构的意义。

## 02 单机架构

我们从最简单的开始讲起。

假设你的业务处于起步阶段，体量非常小，那你的架构是这样的：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072134891.png" alt="image-20220707213457842" style="zoom:50%;" />

这个架构模型非常简单，客户端请求进来，业务应用读写数据库，返回结果，非常好理解。

但需要注意的是，这里的数据库是「单机」部署的，所以它有一个致命的缺点：一旦遭遇意外，例如磁盘损坏、操作系统异常、误删数据，那这意味着所有数据就全部「丢失」了，这个损失是巨大的。

如何避免这个问题呢？我们很容易想到一个方案：**备份**。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072135212.png" alt="image-20220707213516129" style="zoom:50%;" />

你可以对数据做备份，把数据库文件「定期」cp 到另一台机器上，这样，即使原机器丢失数据，你依旧可以通过备份把数据「恢复」回来，以此保证数据安全。

这个方案实施起来虽然比较简单，但存在 2 个问题：

1. **恢复需要时间**：业务需先停机，再恢复数据，停机时间取决于恢复的速度，恢复期间服务「不可用」
2. **数据不完整**：因为是定期备份，数据肯定不是「最新」的，数据完整程度取决于备份的周期

很明显，你的数据库越大，意味故障恢复时间越久。那按照前面我们提到的「高可用」标准，这个方案可能连 1 个 9 都达不到，远远无法满足我们对可用性的要求。

那有什么更好的方案，既可以快速恢复业务？还能尽可能保证数据完整性呢？

这时你可以采用这个方案：**主从副本**。

## 03 主从副本

你可以在另一台机器上，再部署一个数据库实例，让这个新实例成为原实例的「副本」，让两者保持「实时同步」，就像这样：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072136257.png" alt="image-20220707213613177" style="zoom:50%;" />

我们一般把原实例叫作主库（master），新实例叫作从库（slave）。这个方案的优点在于：

- **数据完整性高**：主从副本实时同步，数据「差异」很小
- **抗故障能力提升**：主库有任何异常，从库可随时「切换」为主库，继续提供服务
- **读性能提升**：业务应用可直接读从库，分担主库「压力」读压力

这个方案不错，不仅大大提高了数据库的可用性，还提升了系统的读性能。

同样的思路，你的「业务应用」也可以在其它机器部署一份，避免单点。因为业务应用通常是「无状态」的（不像数据库那样存储数据），所以直接部署即可，非常简单。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072140054.png" alt="image-20220707214041974" style="zoom:50%;" />

因为业务应用部署了多个，所以你现在还需要部署一个「接入层」，来做请求的「负载均衡」（一般会使用 nginx 或 LVS），这样当一台机器宕机后，另一台机器也可以「接管」所有流量，持续提供服务。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072141729.png" alt="image-20220707214101633" style="zoom:50%;" />

从这个方案你可以看出，提升可用性的关键思路就是：**冗余**。

没错，担心一个实例故障，那就部署多个实例，担心一个机器宕机，那就部署多台机器。

到这里，你的架构基本已演变成主流方案了，之后开发新的业务应用，都可以按照这种模式去部署。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072141867.png" alt="image-20220707214135743" style="zoom:50%;" />

但这种方案还有什么风险吗？

## 04 风险不可控

现在让我们把视角下放，把焦点放到具体的「部署细节」上来。

按照前面的分析，为了避免单点故障，你的应用虽然部署了多台机器，但这些机器的分布情况，我们并没有去深究。

而一个机房有很多服务器，这些服务器通常会分布在一个个「机柜」上，如果你使用的这些机器，刚好在一个机柜，还是存在风险。

如果恰好连接这个机柜的交换机 / 路由器发生故障，那么你的应用依旧有「不可用」的风险。

> 虽然交换机 / 路由器也做了路线冗余，但不能保证一定不出问题。

部署在一个机柜有风险，那把这些机器打散，分散到不同机柜上，是不是就没问题了？

这样确实会大大降低出问题的概率。但我们依旧不能掉以轻心，因为无论怎么分散，它们总归还是在一个相同的环境下：**机房**。

那继续追问，机房会不会发生故障呢？

一般来讲，建设一个机房的要求其实是很高的，地理位置、温湿度控制、备用电源等等，机房厂商会在各方面做好防护。但即使这样，我们每隔一段时间还会看到这样的新闻：

- 2015 年 5 月 27 日，杭州市某地光纤被挖断，近 3 亿用户长达 5 小时无法访问支付宝
- 2021 年 7 月 13 日，B 站部分服务器机房发生故障，造成整站持续 3 个小时无法访问
- 2021 年 10 月 9 日，富途证券服务器机房发生电力闪断故障，造成用户 2 个小时无法登陆、交易
- ...

可见，即使机房级别的防护已经做得足够好，但只要有「概率」出问题，那现实情况就有可能发生。虽然概率很小，但一旦真的发生，影响之大可见一斑。

看到这里你可能会想，机房出现问题的概率也太小了吧，工作了这么多年，也没让我碰上一次，有必要考虑得这么复杂吗？

但你有没有思考这样一个问题：**不同体量的系统，它们各自关注的重点是什么？**

体量很小的系统，它会重点关注「用户」规模、增长，这个阶段获取用户是一切。等用户体量上来了，这个阶段会重点关注「性能」，优化接口响应时间、页面打开速度等等，这个阶段更多是关注用户体验。

等体量再大到一定规模后你会发现，「可用性」就变得尤为重要。像微信、支付宝这种全民级的应用，如果机房发生一次故障，那整个影响范围可以说是非常巨大的。

所以，再小概率的风险，我们在提高系统可用性时，也不能忽视。

分析了风险，再说回我们的架构。那到底该怎么应对机房级别的故障呢？

没错，还是**冗余**。

## 05 同城灾备

想要抵御「机房」级别的风险，那应对方案就不能局限在一个机房内了。

现在，你需要做机房级别的冗余方案，也就是说，你需要再搭建一个机房，来部署你的服务。

简单起见，你可以在「同一个城市」再搭建一个机房，原机房我们叫作 A 机房，新机房叫 B 机房，这两个机房的网络用一条「专线」连通。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072142188.png" alt="image-20220707214258065" style="zoom:50%;" />

有了新机房，怎么把它用起来呢？这里还是要优先考虑「数据」风险。

为了避免 A 机房故障导致数据丢失，所以我们需要把数据在 B 机房也存一份。最简单的方案还是和前面提到的一样：**备份**。

A 机房的数据，定时在 B 机房做备份（拷贝数据文件），这样即使整个 A 机房遭到严重的损坏，B 机房的数据不会丢，通过备份可以把数据「恢复」回来，重启服务。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072143942.png" alt="image-20220707214329825" style="zoom:50%;" />

这种方案，我们称之为「**冷备**」。为什么叫冷备呢？因为 B 机房只做备份，不提供实时服务，它是冷的，只会在 A 机房故障时才会启用。

但备份的问题依旧和之前描述的一样：数据不完整、恢复数据期间业务不可用，整个系统的可用性还是无法得到保证。

所以，我们还是需要用「主从副本」的方式，在 B 机房部署 A 机房的数据副本，架构就变成了这样：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072144251.png" alt="image-20220707214443131" style="zoom:50%;" />

这样，就算整个 A 机房挂掉，我们在 B 机房也有比较「完整」的数据。

数据是保住了，但这时你需要考虑另外一个问题：**如果 A 机房真挂掉了，要想保证服务不中断，你还需要在 B 机房「紧急」做这些事情**：

1. B 机房所有从库提升为主库
2. 在 B 机房部署应用，启动服务
3. 部署接入层，配置转发规则
4. DNS 指向 B 机房，接入流量，业务恢复

看到了么？A 机房故障后，B 机房需要做这么多工作，你的业务才能完全「恢复」过来。

你看，整个过程需要人为介入，且需花费大量时间来操作，恢复之前整个服务还是不可用的，这个方案还是不太爽，如果能做到故障后立即「切换」，那就好了。

因此，要想缩短业务恢复的时间，你必须把这些工作在 B 机房「**提前**」做好，也就是说，你需要在 B 机房提前部署好接入层、业务应用，等待随时切换。架构就变成了这样：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072145478.png" alt="image-20220707214511338" style="zoom: 50%;" />

这样的话，A 机房整个挂掉，我们只需要做 2 件事即可：

1. B 机房所有从库提升为主库
2. DNS 指向 B 机房，接入流量，业务恢复

这样一来，恢复速度快了很多。

到这里你会发现，B 机房从最开始的「空空如也」，演变到现在，几乎是「**镜像**」了一份 A 机房的所有东西，从最上层的接入层，到中间的业务应用，到最下层的存储。

两个机房唯一的区别是，**A 机房的存储都是主库，而 B 机房都是从库**。

这种方案，我们把它叫做「**热备**」。

热的意思是指，B 机房处于「待命」状态，A 故障后 B 可以随时「接管」流量，继续提供服务。热备相比于冷备最大的优点是：**随时可切换**。

无论是冷备还是热备，因为它们都处于「备用」状态，所以我们把这两个方案统称为：**同城灾备**。

同城灾备的最大优势在于，我们再也不用担心「机房」级别的故障了，一个机房发生风险，我们只需把流量切换到另一个机房即可，可用性再次提高，是不是很爽？（后面还有更爽的）

## 06 同城双活

高性能的同城双活，核心思想就是避免跨机房调用：

保证同机房服务调用：不同的 PRC（远程调用） 服务，向注册中心注册不同的服务组，而 RPC 服务只订阅同机房的 RPC 服务组，RPC 调用只存在于本机房。

保证同机房缓存调用：查询缓存发生在本机房，如果没有，则从数据库加载。缓存也是采用主备的方式，数据更新采用多机房更新的方式。

保证同机房数据库查询：和缓存一样，读取本机房的数据库，同样采用主备方式。

我们继续来看这个架构。

虽然我们有了应对机房故障的解决方案，但这里有个问题是我们不能忽视的：**A 机房挂掉，全部流量切到 B 机房，B 机房能否真的如我们所愿，正常提供服务？**

这是个值得思考的问题。

这就好比有两支军队 A 和 B，A 军队历经沙场，作战经验丰富，而 B 军队只是后备军，除了有军人的基本素养之外，并没有实战经验，战斗经验基本为 0。

如果 A 军队丧失战斗能力，需要 B 军队立即顶上时，作为指挥官的你，肯定也会担心 B 军队能否真的担此重任吧？

我们的架构也是如此，此时的 B 机房虽然是随时「待命」状态，但 A 机房真的发生故障，我们要把全部流量切到 B 机房，其实是不敢百分百保证它可以「如期」工作的。

你想，我们在一个机房内部署服务，还总是发生各种各样的问题，例如：发布应用的版本不一致、系统资源不足、操作系统参数不一样等等。现在多部署一个机房，这些问题只会增多，不会减少。

另外，从「成本」的角度来看，我们新部署一个机房，需要购买服务器、内存、硬盘、带宽资源，花费成本也是非常高昂的，只让它当一个后备军，未免也太「大材小用」了！

因此，我们需要让 B 机房也接入流量，实时提供服务，这样做的好处，**一是可以实时训练这支后备军，让它达到与 A 机房相同的作战水平，随时可切换，二是 B 机房接入流量后，可以分担 A 机房的流量压力**。这才是把 B 机房资源优势，发挥最大化的最好方案！

那怎么让 B 机房也接入流量呢？很简单，就是把 B 机房的接入层 IP 地址，加入到 DNS 中，这样，B 机房从上层就可以有流量进来了。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072146457.png" alt="image-20220707214612352" style="zoom:50%;" />

但这里有一个问题：别忘了，B 机房的存储，现在可都是 A 机房的「从库」，从库默认可都是「不可写」的，B 机房的写请求打到本机房存储上，肯定会报错，这还是不符合我们预期。怎么办？

这时，你就需要在「业务应用」层做改造了。

你的业务应用在操作数据库时，需要区分「**读写分离**」（一般用中间件实现），即两个机房的「读」流量，可以读任意机房的存储，但「写」流量，只允许写 A 机房，因为主库在 A 机房。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072146747.png" alt="image-20220707214637650" style="zoom:50%;" />

这会涉及到你用的所有存储，例如项目中用到了 MySQL、Redis、MongoDB 等等，操作这些数据库，都需要区分读写请求，所以这块需要一定的业务「改造」成本。

因为 A 机房的存储都是主库，所以我们把 A 机房叫做「**主机房**」，B 机房叫「**从机房**」。

两个机房部署在「同城」，物理距离比较近，而且两个机房用「专线」网络连接，虽然跨机房访问的延迟，比单个机房内要大一些，但整体的延迟还是可以接受的。

业务改造完成后，B 机房可以慢慢接入流量，从 10%、30%、50% 逐渐覆盖到 100%，你可以持续观察 B 机房的业务是否存在问题，有问题及时修复，逐渐让 B 机房的工作能力，达到和 A 机房相同水平。

现在，因为 B 机房实时接入了流量，此时如果 A 机房挂了，那我们就可以「大胆」地把 A 的流量，全部切换到 B 机房，完成快速切换！

到这里你可以看到，我们部署的 B 机房，在物理上虽然与 A 有一定距离，但整个系统从「逻辑」上来看，我们是把这两个机房看做一个「整体」来规划的，也就是说，相当于把 2 个机房当作 1 个机房来用。

这种架构方案，比前面的同城灾备更「进了一步」，B 机房实时接入了流量，还能应对随时的故障切换，这种方案我们把它叫做「**同城双活**」。

因为两个机房都能处理业务请求，这对我们系统的内部维护、改造、升级提供了更多的可实施空间（流量随时切换），现在，整个系统的弹性也变大了，是不是更爽了？

那这种架构有什么问题呢？

## 07 两地三中心

这个概念也被业界提到过很多次。

两地：本地和异地。

三中心：本地数据中心、同城数据中心、异地数据中心。

这两个概念也就是我上面说的同城双活和异地多活的方式，只是针对的是数据中心。原理如下图所示：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208221020946.png" alt="image-20220822102000880" style="zoom:67%;" />

还是回到风险上来说。

虽然我们把 2 个机房当做一个整体来规划，但这 2 个机房在物理层面上，还是处于「一个城市」内，如果是整个城市发生自然灾害，例如地震、水灾（河南水灾刚过去不久），那 2 个机房依旧存在「全局覆没」的风险。

真是防不胜防啊？怎么办？没办法，继续冗余。

但这次冗余机房，就不能部署在同一个城市了，你需要把它放到距离更远的地方，部署在「异地」。

> 通常建议两个机房的距离要在 1000 公里以上，这样才能应对城市级别的灾难。

假设之前的 A、B 机房在北京，那这次新部署的 C 机房可以放在上海。

按照前面的思路，把 C 机房用起来，最简单粗暴的方案还就是做「冷备」，即定时把 A、B 机房的数据，在 C 机房做备份，防止数据丢失。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072147564.png" alt="image-20220707214731436" style="zoom:50%;" />

这种方案，就是我们经常听到的「**两地三中心**」。

**两地是指 2 个城市，三中心是指有 3 个机房，其中 2 个机房在同一个城市，并且同时提供服务，第 3 个机房部署在异地，只做数据灾备。**

这种架构方案，通常用在银行、金融、政企相关的项目中。它的问题还是前面所说的，启用灾备机房需要时间，而且启用后的服务，不确定能否如期工作。

所以，要想真正的抵御城市级别的故障，越来越多的互联网公司，开始实施「**异地双活**」。

## 08 伪异地双活

这里，我们还是分析 2 个机房的架构情况。我们不再把 A、B 机房部署在同一个城市，而是分开部署，例如 A 机房放在北京，B 机房放在上海。

前面我们讲了同城双活，那异地双活是不是直接「照搬」同城双活的模式去部署就可以了呢？

事情没你想的那么简单。

如果还是按照同城双活的架构来部署，那异地双活的架构就是这样的：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072148068.png" alt="image-20220707214827956" style="zoom:50%;" />

注意看，两个机房的网络是通过「**跨城专线**」连通的。

此时两个机房都接入流量，那上海机房的请求，可能要去读写北京机房的存储，这里存在一个很大的问题：**网络延迟**。

因为两个机房距离较远，受到物理距离的限制，现在，两地之间的网络延迟就变成了「**不可忽视**」的因素了。

北京到上海的距离大约 1300 公里，即使架设一条高速的「网络专线」，光纤以光速传输，一个来回也需要近 10ms 的延迟。

况且，网络线路之间还会经历各种路由器、交换机等网络设备，实际延迟可能会达到 30ms ~ 100ms，如果网络发生抖动，延迟甚至会达到 1 秒。

> 不止是延迟，远距离的网络专线质量，是远远达不到机房内网络质量的，专线网络经常会发生延迟、丢包、甚至中断的情况。总之，不能过度信任和依赖「跨城专线」。

你可能会问，这点延迟对业务影响很大吗？影响非常大！

试想，一个客户端请求打到上海机房，上海机房要去读写北京机房的存储，一次跨机房访问延迟就达到了 30ms，这大致是机房内网网络（0.5 ms）访问速度的 60 倍（30ms / 0.5ms），一次请求慢 60 倍，来回往返就要慢 100 倍以上。

而我们在 App 打开一个页面，可能会访问后端几十个 API，每次都跨机房访问，整个页面的响应延迟有可能就达到了**秒级**，这个性能简直惨不忍睹，难以接受。

看到了么，虽然我们只是简单的把机房部署在了「异地」，但「同城双活」的架构模型，在这里就不适用了，还是按照这种方式部署，这是「伪异地双活」！

那如何做到真正的异地双活呢？

## 09 真正的异地双活

既然「跨机房」调用延迟是不容忽视的因素，那我们只能尽量避免跨机房「调用」，规避这个延迟问题。

也就是说，上海机房的应用，不能再「跨机房」去读写北京机房的存储，只允许读写上海本地的存储，实现「就近访问」，这样才能避免延迟问题。

还是之前提到的问题：上海机房存储都是从库，不允许写入啊，除非我们只允许上海机房接入「读流量」，不接收「写流量」，否则无法满足不再跨机房的要求。

很显然，只让上海机房接收读流量的方案不现实，因为很少有项目是只有读流量，没有写流量的。所以这种方案还是不行，这怎么办？

此时，你就必须在「**存储层**」做改造了。

要想上海机房读写本机房的存储，那上海机房的存储不能再是北京机房的从库，而是也要变为「主库」。

你没看错，两个机房的存储必须都是「**主库**」，而且两个机房的数据还要「**互相同步**」数据，即客户端无论写哪一个机房，都能把这条数据同步到另一个机房。

因为只有两个机房都拥有「全量数据」，才能支持任意切换机房，持续提供服务。

怎么实现这种「双主」架构呢？它们之间如何互相同步数据？

如果你对 MySQL 有所了解，MySQL 本身就提供了双主架构，它支持双向复制数据，但平时用的并不多。而且 Redis、MongoDB 等数据库并没有提供这个功能，所以，你必须开发对应的「**数据同步中间件**」来实现双向同步的功能。

此外，除了数据库这种有状态的软件之外，你的项目通常还会使用到消息队列，例如 RabbitMQ、Kafka，这些也是有状态的服务，所以它们也需要开发双向同步的中间件，支持任意机房写入数据，同步至另一个机房。

看到了么，这一下子复杂度就上来了，单单针对每个数据库、队列开发同步中间件，就需要投入很大精力了。

> 业界也开源出了很多数据同步中间件，例如阿里的 Canal、RedisShake、MongoShake，可分别在两个机房同步 MySQL、Redis、MongoDB 数据。
>
> 很多有能力的公司，也会采用自研同步中间件的方式来做，例如饿了么、携程、美团都开发了自己的同步中间件。
>
> 我也有幸参与设计开发了 MySQL、Redis/Codis、MongoDB 的同步中间件，有时间写一篇文章详细聊聊实现细节，欢迎持续关注。:)

现在，整个架构就变成了这样：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072153586.png" alt="image-20220707215330478" style="zoom:50%;" />

注意看，两个机房的存储层都互相同步数据的。有了数据同步中间件，就可以达到这样的效果：

- 北京机房写入 X = 1
- 上海机房写入 Y = 2
- 数据通过中间件双向同步
- 北京、上海机房都有 X = 1、Y = 2 的数据

这里我们用中间件双向同步数据，就不用再担心专线问题，专线出问题，我们的中间件可以自动重试，直到成功，达到数据最终一致。

但这里还会遇到一个问题，两个机房都可以写，操作的不是同一条数据那还好，如果修改的是同一条的数据，发生冲突怎么办？

- 用户短时间内发了 2 个修改请求，都是修改同一条数据
- 一个请求落在北京机房，修改 X = 1（还未同步到上海机房）
- 另一个请求落在上海机房，修改 X = 2（还未同步到北京机房）
- 两个机房以哪个为准？

也就是说，在很短的时间内，同一个用户修改同一条数据，两个机房无法确认谁先谁后，数据发生「冲突」。

这是一个很严重的问题，系统发生故障并不可怕，可怕的是数据发生「错误」，因为修正数据的成本太高了。我们一定要避免这种情况的发生。解决这个问题，有 2 个方案。

**第一个方案**，数据同步中间件要有自动「合并」数据、解决「冲突」的能力。

这个方案实现起来比较复杂，要想合并数据，就必须要区分出「先后」顺序。我们很容易想到的方案，就是以「时间」为标尺，以「后到达」的请求为准。

但这种方案需要两个机房的「时钟」严格保持一致才行，否则很容易出现问题。例如：

- 第 1 个请求落到北京机房，北京机房时钟是 10:01，修改 X = 1
- 第 2 个请求落到上海机房，上海机房时钟是 10:00，修改 X = 2

因为北京机房的时间「更晚」，那最终结果就会是 X = 1。但这里其实应该以第 2 个请求为准，X = 2 才对。

可见，完全「依赖」时钟的冲突解决方案，不太严谨。

所以，通常会采用**第二种方案**，从「**源头**」就避免数据冲突的发生。

## 10 如何实施异地双活

既然自动合并数据的方案实现成本高，那我们就要想，能否从源头就「避免」数据冲突呢？

这个思路非常棒！

从源头避免数据冲突的思路是：**在最上层接入流量时，就不要让冲突的情况发生。**

具体来讲就是，要在最上层就把用户「区分」开，部分用户请求固定打到北京机房，其它用户请求固定打到上海 机房，进入某个机房的用户请求，之后的所有业务操作，都在这一个机房内完成，从根源上避免「跨机房」。

所以这时，你需要在接入层之上，再部署一个「路由层」（通常部署在云服务器上），自己可以配置路由规则，把用户「分流」到不同的机房内。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072151014.png" alt="image-20220707215145904" style="zoom:50%;" />

但这个路由规则，具体怎么定呢？有很多种实现方式，最常见的我总结了 3 类：

1. 按业务类型分片
2. 直接哈希分片
3. 按地理位置分片

**1、按业务类型分片**

这种方案是指，按应用的「业务类型」来划分。

举例：假设我们一共有 4 个应用，北京和上海机房都部署这些应用。但应用 1、2 只在北京机房接入流量，在上海机房只是热备。应用 3、4 只在上海机房接入流量，在北京机房是热备。

这样一来，应用 1、2 的所有业务请求，只读写北京机房存储，应用 3、4 的所有请求，只会读写上海机房存储。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072152233.png" alt="image-20220707215216118" style="zoom:50%;" />

这样按业务类型分片，也可以避免同一个用户修改同一条数据。

> 这里按业务类型在不同机房接入流量，还需要考虑多个应用之间的依赖关系，要尽可能的把完成「相关」业务的应用部署在同一个机房，避免跨机房调用。
>
> 例如，订单、支付服务有依赖关系，会产生互相调用，那这 2 个服务在 A 机房接入流量。社区、发帖服务有依赖关系，那这 2 个服务在 B 机房接入流量。

**2、直接哈希分片**

这种方案就是，最上层的路由层，会根据用户 ID 计算「哈希」取模，然后从路由表中找到对应的机房，之后把请求转发到指定机房内。

举例：一共 200 个用户，根据用户 ID 计算哈希值，然后根据路由规则，把用户 1 - 100 路由到北京机房，101 - 200 用户路由到上海机房，这样，就避免了同一个用户修改同一条数据的情况发生。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072152821.png" alt="image-20220707215240709" style="zoom:50%;" />



**3、按地理位置分片**

这种方案，非常适合与地理位置密切相关的业务，例如打车、外卖服务就非常适合这种方案。

拿外卖服务举例，你要点外卖肯定是「就近」点餐，整个业务范围相关的有商家、用户、骑手，它们都是在相同的地理位置内的。

针对这种特征，就可以在最上层，按用户的「地理位置」来做分片，分散到不同的机房。

举例：北京、河北地区的用户点餐，请求只会打到北京机房，而上海、浙江地区的用户，请求则只会打到上海机房。这样的分片规则，也能避免数据冲突。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072153389.png" alt="image-20220707215302269" style="zoom:50%;" />

> 提醒：这 3 种常见的分片规则，第一次看不太好理解，建议配合图多理解几遍。搞懂这 3 个分片规则，你才能真正明白怎么做异地多活。

总之，分片的核心思路在于，**让同一个用户的相关请求，只在一个机房内完成所有业务「闭环」，不再出现「跨机房」访问。**

阿里在实施这种方案时，给它起了个名字，叫做「**单元化**」。

> 当然，最上层的路由层把用户分片后，理论来说同一个用户只会落在同一个机房内，但不排除程序 Bug 导致用户会在两个机房「漂移」。
>
> 安全起见，每个机房在写存储时，还需要有一套机制，能够检测「数据归属」，应用层操作存储时，需要通过中间件来做「兜底」，避免不该写本机房的情况发生。（篇幅限制，这里不展开讲，理解思路即可）

现在，两个机房就可以都接收「读写」流量（做好分片的请求），底层存储保持「双向」同步，两个机房都拥有全量数据，当任意机房故障时，另一个机房就可以「接管」全部流量，实现快速切换，简直不要太爽。

不仅如此，因为机房部署在异地，我们还可以更细化地「优化」路由规则，让用户访问就近的机房，这样整个系统的性能也会大大提升。

> 这里还有一种情况，是无法做数据分片的：**全局数据**。例如系统配置、商品库存这类需要强一致的数据，这类服务依旧只能采用写主机房，读从机房的方案，不做双活。
>
> 双活的重点，是要优先保证「核心」业务先实现双活，并不是「全部」业务实现双活。

至此，我们才算实现了真正的「**异地双活**」！

> 到这里你可以看出，完成这样一套架构，需要投入的成本是巨大的。
>
> 路由规则、路由转发、数据同步中间件、数据校验兜底策略，不仅需要开发强大的中间件，同时还要业务配合改造（业务边界划分、依赖拆分）等一些列工作，没有足够的人力物力，这套架构很难实施。

## 11 异地多活

同城双活无法做到城市级别的容灾。所以需要考虑异地多活。

比如上海的服务器宕机了，还有重庆的服务器可以顶上来。但两地距离不要太近，因为发生自然灾害时有可能会被另外一地波及到。

和同城双活的核心思想一样，避免跨机房调用。但是因为异地方案中的调用延迟远大于同机房的方案，所以数据同步是一个非常值得探讨的点。提供两种方案：

- 基于存储系统的主从复制，MySQL 和 Redis 天生就具备。但是数据量很大的情况下，性能是较差的。
- 异步复制的方式。基于消息队列，将数据操作作为一个消息放到消息队列，另外的机房消费这条消息，操作存储组件。

理解了异地双活，那「异地多活」顾名思义，就是在异地双活的基础上，部署多个机房即可。架构变成了这样：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072149515.png" alt="image-20220707214958438" style="zoom: 67%;" />

这些服务按照「单元化」的部署方式，可以让每个机房部署在任意地区，随时扩展新机房，你只需要在最上层定义好分片规则就好了。

但这里还有一个小问题，随着扩展的机房越来越多，当一个机房写入数据后，需要同步的机房也越来越多，这个实现复杂度会比较高。

所以业界又把这一架构又做了进一步优化，把「网状」架构升级为「星状」：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207072151675.png" alt="image-20220707215122563" style="zoom:67%;" />

这种方案必须设立一个「中心机房」，任意机房写入数据后，都只同步到中心机房，再由中心机房同步至其它机房。

这样做的好处是，一个机房写入数据，只需要同步数据到中心机房即可，不需要再关心一共部署了多少个机房，实现复杂度大大「简化」。

但与此同时，这个中心机房的「稳定性」要求会比较高。不过也还好，即使中心机房发生故障，我们也可以把任意一个机房，提升为中心机房，继续按照之前的架构提供服务。

至此，我们的系统彻底实现了「**异地多活**」！

多活的优势在于，**可以任意扩展机房「就近」部署。任意机房发生故障，可以完成快速「切换」**，大大提高了系统的可用性。

同时，我们也再也不用担心系统规模的增长，因为这套架构具有极强的「**扩展能力**」。

怎么样？我们从一个最简单的应用，一路优化下来，到最终的架构方案，有没有帮你彻底理解异地多活呢？

## 总结

不论使用哪种方式，都涉及到跨机房数据传输延迟的问题。

- 同地多机房专线，延迟 1ms~3 ms。
- 异地多机房专线，延迟 50 ms 左右。
- 跨国多机房，延迟 200 ms 左右。

好了，总结一下这篇文章的重点。

1、一个好的软件架构，应该遵循高性能、高可用、易扩展 3 大原则，其中「高可用」在系统规模变得越来越大时，变得尤为重要

2、系统发生故障并不可怕，能以「最快」的速度恢复，才是高可用追求的目标，异地多活是实现高可用的有效手段

3、提升高可用的核心是「冗余」，备份、主从副本、同城灾备、同城双活、两地三中心、异地双活，异地多活都是在做冗余

4、同城灾备分为「冷备」和「热备」，冷备只备份数据，不提供服务，热备实时同步数据，并做好随时切换的准备

5、同城双活比灾备的优势在于，两个机房都可以接入「读写」流量，提高可用性的同时，还提升了系统性能。虽然物理上是两个机房，但「逻辑」上还是当做一个机房来用

6、两地三中心是在同城双活的基础上，额外部署一个异地机房做「灾备」，用来抵御「城市」级别的灾害，但启用灾备机房需要时间

7、异地双活才是抵御「城市」级别灾害的更好方案，两个机房同时提供服务，故障随时可切换，可用性高。但实现也最复杂，理解了异地双活，才能彻底理解异地多活

8、异地多活是在异地双活的基础上，任意扩展多个机房，不仅又提高了可用性，还能应对更大规模的流量的压力，扩展性最强，是实现高可用的最终方案



## 后记

这篇文章我从「宏观」层面，向你介绍了异地多活架构的「核心」思路，整篇文章的信息量还是很大的，如果不太好理解，我建议你多读几遍。

因为篇幅限制，很多细节我并没有展开来讲。这篇文章更像是讲异地多活的架构之「道」，而真正实施的「术」，要考虑的点其实也非常繁多，因为它需要开发强大的「基础设施」才可以完成实施。

不仅如此，要想真正实现异地多活，还需要遵循一些原则，例如业务梳理、业务分级、数据分类、数据最终一致性保障、机房切换一致性保障、异常处理等等。同时，相关的运维设施、监控体系也要能跟得上才行。

宏观上需要考虑业务（微服务部署、依赖、拆分、SDK、Web 框架）、基础设施（服务发现、流量调度、持续集成、同步中间件、自研存储），微观上要开发各种中间件，还要关注中间件的高性能、高可用、容错能力，其复杂度之高，只有亲身参与过之后才知道。

我曾经有幸参与过，存储层同步中间件的设计与开发，实现过「跨机房」同步 MySQL、Redis、MongoDB 的中间件，踩过的坑也非常多。当然，这些中间件的设计思路也非常有意思，有时间单独分享一下这些中间件的设计思路。

值得提醒你的是，只有真正理解了「异地双活」，才能彻底理解「异地多活」。在我看来，从同城双活演变为异地双活的过程，是最为复杂的，最核心的东西包括，**业务单元化划分、存储层数据双向同步、最上层的分片逻辑**，这些是实现异地多活的重中之重。

# 高可用分析

[你管这破玩意儿叫高可用？ (qq.com)](https://mp.weixin.qq.com/s?__biz=MzU1Nzg4NjgyMw==&mid=2247498143&idx=2&sn=137a83ca7b4d90ad904100944be34d7a&chksm=fc2c4597cb5bcc81d007dc6733e39d622cfbaed0a2f8c1707dcb6a51417a77ee759eb2bc401f&mpshare=1&scene=23&srcid=0511nofXIwVjAaZEqcFMxEHx&sharer_sharetime=1652199731642&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

大家好，今天我们来聊一下互联网三高（高并发、高性能、高可用）中的高可用，看完本文相信能解开你关于高可用设计的大部分困惑。

## 前言

高可用（High availability，即 HA）的主要目的是为了`保障「业务的连续性」`，即在用户眼里，业务永远是正常（或者说基本正常）对外提供服务的。高可用主要是针对架构而言，那么要做好高可用，就要首先设计好架构，第一步我们一般会采用`分层的思想将一个庞大的 IT 系统拆分成为应用层，中间件，数据存储层等独立的层`，每一层再拆分成为更细粒度的组件，第二步就是让每个组件对外提供服务，毕竟每个组件都不是孤立存在的，都需要互相协作，对外提供服务才有意义。

要保证架构的高可用，就要保证架构中所有组件以及其对外暴露服务都要做高可用设计，任何一个组件或其服务没做高可用，都意味着系统存在风险。

那么这么多组件该怎么做高可用设计呢，其实任何组件要做高可用，都离不开`「冗余」和「自动故障转移」`，众所周知`单点是高可用的大敌`，所以组件一般是以集群（至少两台机器）的形式存在的，这样只要某台机器出现问题，集群中的其他机器就可以随时顶替，这就是「冗余」。简单计算一下，`假设一台机器的可用性为 90%，则两台机器组成的集群可用性为 1-0.1*0.1 = 99%，所以显然冗余的机器越多，可用性越高`。

但光有冗余还不够，如果机器出现问题，需要人工切换的话也是费时费力，而且容易出错，所以我们`还需要借助第三方工具（即仲裁者）的力量来实现「自动」的故障转移`，以达到实现**近实时**的故障转移的目的，**近实时的故障转移才是高可用的主要意义**

怎样的系统可以称之为高可用呢，业界一般用几个九来衡量系统的可用性，如下

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131546137.png" alt="image-20220513154643064" style="zoom:80%;" />

一般实现两个 9 很简单，毕竟每天宕机 14 分钟已经严重影响业务了，这样的公司迟早歇菜，大厂一般要求 4 个 9，其他要求严苛的业务要达到五个九以上，比如如果因为一个电脑的故障导致所有列车停驶，那么就会有数以万计的人正常生活受到阻碍，这种情况就要求五个九以上

接下来我们就来一起看看架构中的各个组件如何借助「冗余」和「自动故障转移」来实现高可用。

## 互联网架构剖析

目前多数互联网都会采用微服务架构，常见架构如下:

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131547150.png" alt="image-20220513154753061" style="zoom:80%;" />

可以看到架构主要分以下几层

1. 接入层：主要由 F5 硬件或 LVS 软件来承载所有的流量入口
2. 反向代理层：Nginx，主要负责根据 url 来分发流量，限流等
3. 网关：主要负责流控，风控，协议转换等
4. 站点层：主要负责调用会员，促销等基本服务来装配 json 等数据并返回给客户端
5. 基础 service：其实与站点层都属于微服务，是平级关系，只不过基础 service 属于基础设施，能被上层的各个业务层 server 调用而已
6. 存储层：也就是 DB，如 MySQL，Oracle 等，一般由基础 service 调用返回给站点层
7. 中间件：ZK，ES，Redis，MQ 等，主要起到加速访问数据等功能，在下文中我们会简单介绍下各个组件的作用

如前所述，要实现整体架构的高可用，必须要实现每一层组件的高可用，接下来我们就来分别看一下每一层的组件都是如何实现高可用的

## 接入层&反向代理层

这两层的高可用都和 keepalived 有关，所以我们结合起来一起看

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131549243.png" alt="image-20220513154936156" style="zoom:80%;" />

对外，两个 LVS 以主备的形式对外提供服务，注意只有 master 在工作（即此时的 VIP 在 master 上生效），另外一个 backup 在 master 宕机之后会接管 master 的工作，那么 backup 怎么知道 master 是否正常呢，答案是`通过 keepalived，在主备机器上都装上 keepalived 软件，启动后就会通过心跳检测彼此的健康状况，一旦 master 宕机，keepalived 会检测到，从而 backup 自动转成 master 对外提供服务，此时 VIP 地址（即图中的 115.204.94.139）即在 backup 上生效，也就是我们常说的「IP漂移」`，通过这样的方式即解决了 LVS 的高可用。

keepalived 的心跳检测主要通过发送 ICMP 报文，或者利用 TCP 的端口连接和扫描检测来检测的，同样的，它也可以用来检测 Nginx 暴露的端口，这样的话如果某些 Nginx 不正常 Keepalived 也能检测到并将其从 LVS 能转发的服务列表中剔出。Nginx也能通过端口检测服务健康状态

借用 keepalived 这个第三方工具，同时实现了 LVS 和 Nginx 的高可用，同时在出现故障时也可以将宕机情况发送到对应开发人员的邮箱以让他们及时收到通知处理，确实很方便，Keepalived 应用广泛，下文我们会看到它也可以用在 MySQL 上来实现 MySQL 的高可用。



## 微服务

接下来我们再来看一下「网关」，「站点层」,「基础服务层」，这三者一般就是我们所说的微服务架构组件，当然这些微服务组件还需要通过一些 RPC 框架如 Dubbo 来支撑才能通信，所以微服务要实现高可用，就意味着 dubbo 这些 RPC 框架也要提供支撑微服务高可用的能力，我们就以 dubbo 为例来看下它是如何实现高可用的

我们先来简单地看下 dubbo 的基本架构

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131552784.png" alt="image-20220513155236717" style="zoom:67%;" />

思路也很简单，首先是 Provider（服务提供者）向 Registry（注册中心，如 ZK 或 Nacos 等）注册服务，然后 Consumer（服务消费者）向注册中心订阅和拉取 Provider 服务列表，获取服务列表后，Consumer 就可以根据其负载均衡策略选择其中一个  Provider 来向其发出请求，当其中某个 Provider 不可用（下线或者因为 GC 阻塞等）时，会被注册中心及时监听（通过心跳机制）到，也会及时推送给 Consumer，这样 Consumer 就能将其从可用的 Provider 列表中剔除，也就实现了故障的自动转移，不难看出，注册中心就起到了类似 keepalived 的作用

## 中间件

我们再来看下这些中间件如 ZK，Redis 等是如何实现高可用的呢

### ZooKeeper

上一节微服务中我们提到了注册中心，那我们就以 ZK（ZooKeeper）为例来看看它的高可用是如何实现的，先来看下它的整体架构图如下

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131556288.png" alt="image-20220513155602203" style="zoom:80%;" />

Zookeeper 中的主要角色如下

- Leader: 即领导者，在集群中只有一个 Leader，主要承担了以下的功能

1. `事务请求的唯一调度和处理者，保证集群事务处理的顺序性`，所有 Follower 的写请求都会转给 Leader 执行，用来保证事务的一致性
2. `集群内部各服务器的调度者：处理好事务请求后，会将数据广播同步到各个 Follower，统计 Follower 写入成功的数量，超过半数 Follower 写入成功，Leader 就会认为写请求提交成功，通知所有的 Follower commit 这个写操作，保证事后哪怕是集群崩溃恢复或者重启`，这个写操作也不会丢失。

- Follower:

1. `处理客户端非事务请求、转发事务请求给 leader 服务器`
2. `参与事物请求 Proposal 的投票（需要半数以上服务器通过才能通知 leader commit 数据; Leader 发起的提案，要求 Follower 投票）`
3. 参与 Leader 选举的投票

**画外音**：Zookeeper 3.0 之后新增了一种 Observer 的角色，不过与此处讨论的 ZK 高可用关系不是很大，为了简化问题，所以省略

可以看到由于只有一个 Leader，很显然，此 Leader 存在单点隐患，那么 ZK 是怎么解决此问题的呢，首先 Follower 与 Leader 会用心跳机制保持连接，如果 Leader 出现问题了（宕机或者因为 FullGC 等原因无法响应），Follower 就无法感知到 Leader 的心跳，就会认为 Leader 出问题了，于是它们就会发起投票选举，最终在多个 Follower 中选出一个 Leader 来（这里主要用到了 Zookeeper Atomic Broadcast，即 ZAB 协议，它是为 ZK 专门设计的一种支持崩溃恢复的一致性协议），选举的细节不是本文重点，就不在此详述了。

`除了 ZAB 协议，业界上常用的还有 Paxos，Raft 等协议算法，也可以用在 Leader 选举上，也就是是在分布式架构中，这些协议算法承担了“第三者”也就是仲裁者的作用，以承担故障的自动转移`



### Redis

Redis 的高可用需要根据它的部署模式来看看，主要分为「主从模式」和「Cluster 分片模式」两种

#### 主从模式

先来看一下主从模式，架构如下

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131558789.png" alt="image-20220513155811708" style="zoom:67%;" />

主从模式即一主多从（一个或者多个从节点），其`中主节点主要负责读和写，然后会将数据同步到多个从节点上，Client 也可以对多个从节点发起读请求，这样可以减轻主节点的压力`，但和 ZK 一样，由于只有一个主节点，存在单点隐患，所以必须引入第三方仲裁者的机制来判定主节点是否宕机以及在判定主节点宕机后快速选出某个从节点来充当主节点的角色，这个第三方仲裁者在 Redis 中我们一般称其为「哨兵」（sentinel），当然哨兵进程本身也有可能挂掉，所以为了安全起见，需要部署多个哨兵（即哨兵集群）

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131558455.png" alt="image-20220513155845360" style="zoom:50%;" />

这些哨兵通过 gossip（流言） 协议来接收关于主服务器是否下线的信息，并在判定主节点宕机后使用 Raft 协议来选举出新的主节点

#### Cluster 分片集群

主从模式看似完美，但存在以下几个问题

1. 主节点写的压力难以降低：因为只有一个主节点能接收写请求，如果在高并发的情况下，写请求如果很高的话可能会把主节点的网卡打满，造成主节点对外无法服务
2. 主节点的存储能力受到单机存储容量的限制：因为不管是主节点还是从节点，存储的都是**全量**缓存数据，那么随着业务量的增长，缓存数据很可能直线上升，直到达到存储瓶颈
3. 同步风暴：因为数据都是从 master 同步到 slave 的，如果有多个从节点的话，master 节点的压力会很大

为了解决主从模式的以上问题，分片集群应运而生，所谓分片集群即将数据分片，每一个分片数据由相应的主节点负责读写，这样的话就有多个主节点来分担写的压力，并且每个节点只存储**部分数据**，也就解决了单机存储瓶颈的问题，但需要注意的是每个主节点都存在单点问题，所以需要针对每个主节点做高可用，整体架构如下

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131600279.png" alt="image-20220513160051180" style="zoom:67%;" />



原理也很简单，在 Proxy 收到 client 执行的 redis 的读写命令后，首先会对 key 进行计算得出一个值，如果这个值落在相应 master 负责的数值范围（一般将每个数字称为槽，Redis 一共有 16384 个槽）之内，那就把这条 redis 命令发给对应的 master 去执行，可以看到每个 master 节点只负责处理一部分的 redis 数据，同时为了避免每个 master 的单点问题，也为其配备了多个从节点以组成集群，当主节点宕机时，集群会通过 Raft 算法来从从节点中选举出一个主节点

### ElasticSearch

再来看一下 ES 是如何实现高可用的，在 ES 中，数据是以分片（Shard）的形式存在的，如下图所示，一个节点中索引数据共分为三个分片存储

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131602879.png" alt="image-20220513160216809" style="zoom:80%;" />

但只有一个节点的话，显然存在和 Redis 的主从架构一样的单点问题，这个节点挂了，ES 也就挂了，所以显然需要创建多个节点

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131602397.png" alt="image-20220513160235312" style="zoom:67%;" />

一旦创建了多个节点，分片（图中 P 为主分片，R 为副本分片）的优势就体现出来了，可以将分片数据分布式存储到其它节点上，极大提升了数据的水平扩展能力，同时每个节点都能承担读写请求，采用负载均衡的形式避免了单点的读写压力

> ES 的写机制与 Redis 和 MySQL 的主从架构有些差别（后两者的写都是直接向 master 节点发起写请求，而 ES 则不是），所以这里稍微解释一下 ES 的工作原理
>
> 首先说下节点的工作机制，节点（Node）分为主节点（Master Node）和从结点（Slave Node），主节点的主要职责是负责集群层面的相关操作，管理集群变更，如创建或删除索引，跟踪哪些节点是集群的一部分，并决定哪些分片分配给相关的节点，主节点也只有一个，一般通过类 Bully 算法来选举出来，如果主节点不可用了，则其他从节点也可以通过此算法来选举以实现集群的高可用，任何节点都可以接收读写请求以达到负载均衡的目的
>
> 再说一下分片的工作原理，分片分为主分片（Primary Shard，即图中 P0，P1，P2）和副本分片（Replica Shard，即图中 R0，R1，R2），主分片负责数据的写操作，所以虽然任何节点可以接收读写请求，但如果此节点接收的是写请求并且没有写数据所在的主分片话，此节点会将写请求调度到主分片所在的节点上，写入主分片后，主分片再把数据复制到其他节点的副本分片上，以有两个副本的集群为例，写操作如下

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131604600.png" alt="image-20220513160400528" style="zoom:67%;" />



### MQ

ES 利用数据分片来提升高可用和水平扩展能力的思想也应用在其他组件的架构设计上，我们以 MQ 中的 Kafka 为例再来看下数据分片的应用

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131604249.png" alt="image-20220513160439148" style="zoom:67%;" />

如上是 Kafka 集群，可以看到每个 Topic 的 Partition 都分布式存储在其它消息服务器上，这样一旦某个 Partition 不可用，可以从 follower 中选举出 leader 继续服务，不过与 ES 中的数据分片不同的是，follower Partition 属于**冷备**，也就是说在正常情况下不会对外服务，只有在 leader 挂掉之后从 follower 中选举出 leader 后它才能对外提供服务

## 存储层

接下来我们再来看一下最后一层，存储层（DB），这里我们以 MySQL 为例来简单地讨论一下其高可用设计，其实大家如果看完了以上的高可用设计，会发现 MySQL 的高可用也不过如此，思想都是类似的，与 Redis 类似，它也分主从和分片（即我们常说的分库分表）两种架构

主从的话与 LVS 类似，一般使用 keepalived 的形式来实现高可用，如下所示

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131607111.png" alt="image-20220513160723030" style="zoom:67%;" />

如果 master 宕机了，Keepalived 也会及时发现，于是从库会升级主库，并且 VIP 也会“漂移”到原从库上生效，所以说大家在工程配置的 MySQL 地址一般是 VIP 以保证高可用

数据量大了之后就要分库分表了，于是就有了多主，就像 Redis 的分片集群一样，需要针对每个主配备多个从，如下

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131611841.png" alt="image-20220513161139765" style="zoom:80%;" />

之前有读者问分库分表之后为啥还要做主从，现在我想大家应该都明白了，不是为了解决读写性能问题，主要是为了实现高可用



## 总结

看完了架构层面的高可用设计，相信大家对高可用的核心思想`「冗余」和「自动故障转移」`会有更深刻的体会，观察以上架构中的组件你会发现冗余的主要原因是因为只有一主，`为什么不能有多主呢，也不是不可以，但这样在分布式系统下要保证数据的一致性是非常困难的，尤其是节点多了的话，数据之间的同步更是一大难题，所以多数组件采用一主的形式，然后再在主和多从之间同步，多数组件之所以选择一主本质上是技术上的 tradeoff`

那么做好每个组件的高可用之后是否整个架构就真的可用了呢，非也，这只能说迈出了第一步，在生产上还有很多突发情况会让我们的系统面临挑战，比如

1. `瞬时流量问题`：比如我们可能会面临秒杀带来的瞬时流量激增导致系统的承载能力被压垮，这种情况可能影响日常交易等核心链路，所以需要做到系统之间的隔离，如单独为秒杀部署一套独立的集群
2. `安全问题`：比如 DDOS 攻击，爬虫频繁请求甚至删库跑路等导致系统拒绝服务
3. `代码问题`：比如代码 bug 引起内存泄露导致 FullGC 导致系统无法响应等
4. `部署问题`：在发布过程中如果贸然中止当前正在运行的服务也是不行的，需要做到优雅停机，平滑发布
5. `第三方问题`：比如我们之前的服务依赖第三方系统，第三方可能出问题导致影响我们的核心业务
6. `不可抗力`：如机房断电，所以需要做好容灾，异地多活，之前我司业务就由于机房故障导致服务四小时不可用，损失惨重

所以除了做好架构的高可用之外，我们还需要在做好`系统隔离，限流，熔断，风控，降级，`对关键操作限制操作人权限等措施以保证系统的可用。

这里`特别提一下降级`，这是为了保证系统可用性采取的常用的措施，简单举几个例子

1. 我们之前对接过一个第三方资金方由于自身原因借款功能出了问题导致无法借款，这种情况为了避免引起用户恐慌，于是我们在用户申请第三方借款的时候返回了一个类似`「为了提升你的额度，资金方正在系统升级」`这样的文案，避免了客诉
2. 在流媒体领域，`当用户观看直播出现严重卡顿时，很多企业的第一选择不是查 log 排查问题，而是为用户自动降码率。因为比起画质降低，卡得看不了显然会让用户更痛苦`
3. 双十一零点高峰期，我们`把用户的注册登录等非核心功能给停掉了，以保证下单等核心流程的顺利`

另外我们最好能做到`事前防御`，在系统出问题前把它扼杀在摇篮里，所以我们需要做单元测试，做`全链路压测等来发现问题，还需要针对 CPU，线程数等做好**监控**，当其达到我们设定的域值时就触发告警以让我们及时发现修复问题`（我司之前就碰到过一个类似的[生产事故复盘](https://mp.weixin.qq.com/s?__biz=MzI5MTU1MzM3MQ==&mid=2247483844&idx=1&sn=549aabbf5ba5e5f03a634411c630a6da&scene=21#wechat_redirect)大家可以看一下），此外在做好单元测试的前提下，依然有可能因为代码的潜在 bug 引起线上问题，所以我们需要在关键时间（比如双十一期间）封网（也就是不让发布代码）

此外我们还需要在出事后能快速定位问题，快速回滚，这就需要记录每一次的发布时间，发布人等，这里的发布不仅包括工程的发布，还包括配置中心等的发布

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131622688.png" alt="image-20220513162211607" style="zoom:80%;" />

**画外音**：上图是我司的发布记录，可以看到有代码变更，回滚等，这样如果发现有问题的话可以一键回滚

最后我们以一张图来总结一下高可用的常见手段

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205131616817.png" alt="image-20220513161638712" style="zoom:80%;" />



# 服务雪崩

话说东汉末年，曹操、孙权、刘备在赤壁市进行了一次争夺老大位置的大战，这就是有名的`赤壁之战`。

## 一、还原赤壁之战

曹操统一北方后，南下打败了刘备，占领荆襄之地后，还想干掉东边的孙权，于是刘备和孙权一起联合抗击曹军八十万大军。

曹操的军队大部分都是北方的，对于水上作战的经验非常欠缺，而且很多士兵晕船，于是曹操命令军队将`船尾用铁索相连`，减弱了风浪颠簸，利于士兵演练。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240929414.png" alt="image-20220824092928335" style="zoom:67%;" />

我们来看看周瑜、黄盖、诸葛亮的对话：

<img src="https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ0UdPhcicMTARoTKVQ8ah7FSbaLhmgcaztUfUVwqGpalGcdHE13eRn9FMC1DE5uraBhJJL5a9LT41Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />

> 黄盖：曹操是真的蠢啊，把船连着，如果船烧着了，其他船会跟着一起烧着的。锁链不易解开，船都逃不了了。我 们用火攻，直接把曹军干趴下。
> 周瑜：但如何接近他们的船呢？
> 黄盖：我用诈降带几艘船出发，船上载浸油的干草，等接近曹军时，点燃干草，冲向曹军的连环船，引燃他们的船
> 周瑜：妙啊！可是哪来的东风？
> 诸葛亮：我来借东风~

赤壁之战那天，火船乘风闯入曹军船阵，顿时一片火海。联军乘势攻击，曹军伤亡惨重，最后以联军大胜结束。以少胜多的经典战役。



## 二、战情分析

周瑜和黄盖看出了连环船的弱点：**如果一只船被烧着了，也会把连着的船烧着** 。

这就很像我们的系统中出现的`服务雪崩`问题。

假定我们系统引进了微服务的思想，将多个服务进行拆分，每个服务都是通过接口调用来完成的，看似功能通过微服务化后，功能和职责单一，正是我们想要的.

但随着业务的增长，服务的数量也是`随之增多`，逻辑也会`更加复杂`，一个服务的某个逻辑需要`依赖`多个其他服务才能完成。假如一个被依赖的服务不能向上游的服务提供服务，则很可能造成`雪崩效应`，最后导致整个服务`不可访问`。

就像雪山上某一处出现积雪崩塌的现象，慢慢地带动其他片区的积雪崩塌，产生了级联反应，最后造成大片的积雪崩塌，这就是常见的雪崩场景。

**小结：** 一个服务失败，导致整条链路的服务都失败的场景，称为服务雪崩。

那曹军应该怎么避免这个问题呢？别急，后面再看答案。

## 三、系统中的雪崩效应

微服务之间往往采用 RPC 或者 HTTP 调用，一般都会设置调用超时的限制，或者通过失败重试机制来确保服务成功执行。但如果不考虑服务的熔断和限流，还是很容易产生服务雪崩的。下面用例子来讲解下雪崩效应是怎么产生的。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240932798.png" alt="image-20220824093228691" style="zoom:50%;" />

- 我们系统中三个服务：`订单服务`、`商品服务`、`库存服务`。
- **下单场景**：用户下单了一个商品，客户端调用订单服务来生成预付款订单，订单服务调用商品服务查看下单的哪款商品，商品服务调用库存服务判断这款商品是否有库存，如有库存，则可以生成预付款订单。
- 假定因双十一流量暴增，库存服务不可用（如响应超时等），库存服务收到的很多请求都未处理完，它将无法处理更多请求。
- 而上游的商品服务依赖库存服务，商品服务的超时和重试机制会被执行。商品服务新的调用不断产生，会导致商品服务的调用被大量积压，产生大量的调用等待和重试调用，慢慢耗尽商品服务的资源，比如内存，结果导致商品服务也宕机了。
- 而订单服务也会重走商品服务的老路。结果就是三个服务都不可用了。

## 四、造成雪崩的真实场景

### 1 服务提供者不可用

- 硬件故障，如网络故障、硬盘损坏等。
- 程序的 bug，如算法需要占用大量 CPU 的计算时间导致 CPU 使用率过高。
- 缓存击穿：比如应用刚重启，短时间内缓存是失效的，导致大量请求直接访问到了数据库，数据库不堪重负，服务不可用。
- 秒杀和大促：服务短时间承载不了那么多请求量。

### 2 重试加大流量

- 用户连续重试，比如用户看到界面上没有响应，所以又操作了一遍，结果又增加了一倍请求量。
- 程序重试机制，比如代码中有多次重试的逻辑，一次失败后，过几秒后再重试，重试个三次就取消重试，走异常处理分支了。也是增加了请求量。

## 五、如何防止雪崩

出问题前预防：限流、主动降级、隔离

出问题后修复：熔断、被动降级

**本篇主要来讲解熔断机制。** 

## 六、熔断原理和算法

### 1 熔断概念

熔断这个概念来源于电路系统中的`保险丝`熔断。当电流过大时，保险丝熔断，防止因`电流过大`损坏电器元器件，或因电流过大，导致元器件热度过高，发生火灾。

**物理公式：** 电功率 P = I^2 * R，I 代表电流，元器件的电阻 R 不变的情况下，电流越大，电功率约大，电阻做的电功大部分都用来`发热`了，所以电功率越大，发热越严重。（还好高中物理没忘。）

放到我们系统中，怎么理解熔断？

如果在某段时间内，调用某个服务非常慢甚至超时，就可以将这个服务熔断，后续其他服务再调用这个服务就直接返回，告诉其他服务：**“已经熔断了，你别调用我了，过段时间再来试下吧。”**

### 2 如何熔断

**熔断有个原则：** 一段时间内，统计失败的次数或者失败请求的占比超过一定阈值，就进行熔断。

详细的原理如下图所示：

<img src="https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ0UdPhcicMTARoTKVQ8ah7FSA5JrcEXkklp6PUiaRicPWneFU1pYWPDUiaKwxXWez4E8QUOgnd7CySPwQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />



### **下面是原理介绍**

### 3 统计请求的算法

- 请求访问到后台服务后，首先判断熔断开关是否打开。
- 如果熔断开关已打开，则表明当前请求不能被处理。
- 如果熔断开关未打开，则判断时间窗口是否已满。
- 如果时间窗口未满，则请求桶中的请求数加 1。
- 如果返回的响应有异常，则失败桶的失败数加 1，如果返回的响应没有异常，则成功桶的成功数加 1。
- 如果时间窗口（判断统计错误率）已满，则开始判断是否需要熔断。

### 4 熔断的恢复算法

- 当熔断后，开关切换到`断开状态`。
- 过一段时间后，开关切换为`半断开状态`（Half-Open）。半断开状态下，允许对应用程序的一定数量的请求可以去调用服务，如果调用成功，则认为服务可以正常访问了，于是将开关切换为`闭合状态`。
- 如果半断开状态下，还是有调用失败的情况，则认为服务还没有恢复，开关从半断开状态切换到`断开状态`。

### 5 统计失败率的时间窗口

时间窗口又分为固定窗口和滑动窗口。

固定时间窗口：

**原理**：固定时间内统计流量总量，超过阀值则限制流量。

**缺陷**：无法限制短时间之内的集中流量。

滑动窗口原理：

**原理**：统计的总时间固定，但时间段是滑动的。

**缺陷**：无法控制流量让它们更加平滑

时间窗口的原理图在这里：

<img src="https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ0UdPhcicMTARoTKVQ8ah7FSS44S3wRh5o6Ejj1vtiaQfPiat7sVRNjaOxibr058JxOiaxVcibBNzG1pIqQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />

- 时间窗口可以比喻为人坐在窗户边，看外面来往的车辆，一定时间内从窗户外经过的车辆。
- 每次请求，都会判断时间窗口是否已满（如5分钟），如果时间窗口已满，则重新开始计时，且清理请求数/成功数/失败数。
- 注意：第一次开始的起始时间默认为当前时间。

### 6 尝试恢复服务的时间窗口

<img src="https://mmbiz.qpic.cn/mmbiz_png/SfAHMuUxqJ0UdPhcicMTARoTKVQ8ah7FSlKndJG1IEQP19sibovvnxKqxSIMLcMLHukSxUKSDf9ERDSrBXlSial9A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />尝试恢复服务的时间窗口

- 开关为断开的状态，经过一定时间后，比如 1 分钟，设置为半断开的状态，尝试发送请求检测服务是否恢复。
- 如果已恢复，则切换状态为关闭状态。如果未恢复，则切换状态为断开的状态，经过 1 分钟后，重复上面的步骤。
- 这里的时间窗口可以根据环境的运行状态进行动态调整，比如第一次是 1 分钟，第二次是 3 分钟，第三次是 10 分钟。

## 七、熔断中间件

肯定有人会问了，你这上面讲的原理，难道还真的自己去写这套算法？

**答案：是的，项目中我们自己造了一个轮子：熔断器。**

但这里我不推荐大家这么做。市面上还有更优秀的开源组件供大家使用，比如阿里系的 `Sentinel`（推荐），Netflix 的 `Hystrix`（已停止更新，维护阶段）。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240935218.png" alt="image-20220824093502143" style="zoom:67%;" />

## 八、扭转战局

曹操大败是因为连锁船的原因，那如何给曹操提供一`妙计`，助他扭转战局呢？

**方案有如下几个：**

- 可以用麻绳代替锁链，因绳子更容易割断。（熔断机制）
- 将船划分到几个区域，区域之间保持一定距离，即使某个区域烧着了，也不会影响其他区域。（熔断+资源隔离）
- 在湖面上提前设关卡，黄盖过来的话，先检查船和人，有问题不予通行。（熔断）



## 九、限流、降级

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240935944.png" alt="image-20220824093529893" style="zoom:67%;" />

对请求的流量进行控制， 只`放行部分请求`，使服务能够承担不超过自己能力的流量压力。

常见限流算法有三种：**时间窗口、漏桶算法、令牌桶算法**。

#### 漏桶算法

原理：按照一个固定的速率将流量露出到接收端。

缺陷：面对突发流量的时候，采用的解决方式是缓存在漏桶中，这样流量的响应时间就会增长，这就与互联网业务低延迟的要求不符。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240935301.png" alt="image-20220824093549239" style="zoom:50%;" />

#### 令牌桶算法

**原理**：一秒内限制访问次数为 N 次。每隔 1/N 的时间，往桶内放入一个令牌。分布式环境下，用 Redis 作为令牌桶。原理图如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240936987.png" alt="image-20220824093610927" style="zoom:67%;" />

总结的思维导图在这里：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208240936843.png" alt="image-20220824093629775" style="zoom:80%;" />





# 负载均衡

## 概念

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207020958559.png" alt="image-20220702095830476" style="zoom: 50%;" />

负载均衡的**两个基本点**：

- 选择哪个服务器来处理客户端请求。
- 将客户端请求转发出去。

**一个核心原理**：通过硬件或软件的方式维护一个服务列表清单。当用户发送请求时，会将请求发送给负载均衡器，然后根据负载均衡算法从可用的服务列表中选出一台服务器的地址，将请求进行转发，完成负载功能。

## 负载均衡的特性

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207020959765.png" alt="image-20220702095914690" style="zoom: 50%;" />

**高性能**：可根据不同的分配规则自动将流量进行分摊。

**可扩展性**：可以很方便增加集群中设备或链路的数量。

**高可靠性**：系统中某个设备或链路发生故障，不会导致服务中断。

**易配置性**：配置和维护方便。

**透明性**：用户感知不到如何进行负载均衡的，也不用关心负载均衡。

## 负载均衡分类

负载均衡技术可以按照软件或硬件进行分类，也可以按照服务器列表存放的位置划分为服务端负载和客户端负载均衡。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207021000921.png" alt="image-20220702100008853" style="zoom:67%;" />



### 1 硬件负载均衡

> F5 就是常见的硬件负载均衡产品。
>
> 优点：性能稳定，具备很多软件负载均衡不具备的功能，如应用交换，会话交换、状态监控等。
>
> 缺点：设备价格昂贵、配置冗余，没有软件负载均衡灵活，不能满足定制化需求。

### 2 软件负载均衡

> Nginx：性能好，可以负载超过 1W。工作在网络的7层之上，可以针对http应用做一些分流的策略。Nginx也可作为静态网页和图片服务器。Nginx仅能支持http、https和Email协议。

> LVS（Linux Virtual Server）：是一个虚拟服务器集群系统，采用 IP 地址均衡技术和内容请求分发技术实现负载均衡。接近硬件设备的网络吞吐和连接负载能力。抗负载能力强、是工作在网络4层之上仅作分发之用。自身有完整的双机热备方案，如LVS+Keepalived。软件本身不支持正则表达式处理，不能做动静分离。

### 3 服务端负载均衡

> Nginx 和 F5 都可以划分到服务端的负载均衡里面，后端的服务器地址列表是存储在后端服务器中或者存在专门的 Nginx 服务器或 F5 上。

> 服务器的地址列表的来源是通过注册中心或者手动配置的方式来的。

### 4 客户端负载均衡

> 终于轮到 Ribbon 登场了，它属于客户端负载均衡器，客户端自己维护一份服务器的地址列表。这个维护的工作就是由 Ribbon 来干的。

> Ribbon 会从 Eureka Server 读取服务信息列表，存储在 Ribbon 中。如果服务器宕机了，Ribbon 会从列表剔除宕机的服务器信息。Ribbon 有多种负载均衡算法，我们可以自行设定规则从而请求到指定的服务器。

































